version: "3.9"

x-common:
  &common
  # 将本地模型目录 映射到容器里面
  volumes:
    - /data/models:/models
  environment:
    &common-env
    TZ: "Asia/Shanghai"

services:
  vllm:
    <<: *common
    image: vllm/vllm-openai:latest
    restart: unless-stopped
    shm_size: '10.24gb'
    ports:
      - "1281:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
      # model 参数修改为自己下载模型的路径， tensor-parallel-size 为使用几张GPU，按实际修改
    command: [ "--model","/models/qwen/Qwen2___5-72B-Instruct-GPTQ-Int8",  "--host", "0.0.0.0", "--port", "8000", "--served-model-name", "gpt-4", "--enable-auto-tool-choice", "--tool-call-parser", "hermes","--distributed-executor-backend","ray","--tensor-parallel-size","4","--pipeline-parallel-size", "1" ]